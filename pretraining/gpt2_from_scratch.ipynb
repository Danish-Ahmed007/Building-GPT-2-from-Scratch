{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu126\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "try:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BZRZLsBWZCFK",
    "outputId": "fca29637-a387-4812-87af-91582d69ca13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in ./jupyter-env/lib/python3.10/site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./jupyter-env/lib/python3.10/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./jupyter-env/lib/python3.10/site-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./jupyter-env/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./jupyter-env/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./jupyter-env/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./jupyter-env/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2025.7.14)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "s7g02P6cZCBA",
    "outputId": "2e4aa00b-4585-40d8-a452-e3e3dedb10cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "importlib.metadata.version(\"tiktoken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pBdltvW7ZB-g"
   },
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AXuAJzW-YNWX"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias = False):\n",
    "    super().__init__()\n",
    "    self.d_out = d_out\n",
    "    self.num_heads = num_heads\n",
    "    self.head_dim = d_out // num_heads\n",
    "\n",
    "    self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "    self.W_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "    self.W_value = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "    self.out_proj = nn.Linear(d_out, d_out)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.register_buffer(\n",
    "        \"mask\",\n",
    "        torch.triu(torch.ones(context_length, context_length), diagonal =1)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    batch, num_tokens, d_in = x.shape\n",
    "    keys = self.W_key(x)\n",
    "    queries = self.W_query(x)\n",
    "    values = self.W_value(x)\n",
    "\n",
    "\n",
    "    keys = keys.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
    "    values = values.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
    "    queries = queries.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "    keys = keys.transpose(1,2)\n",
    "    values = values.transpose(1,2)\n",
    "    queries = queries.transpose(1,2)\n",
    "\n",
    "    attn_scores = queries @ keys.transpose(2,3)\n",
    "\n",
    "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "    attn_scores = attn_scores.masked_fill(mask_bool, -torch.inf) \n",
    "\n",
    "    attn_weights = torch.softmax(attn_scores/ keys.shape[-1]**0.5 , dim=-1)\n",
    "    attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "    context_vecs = (attn_weights @ values).transpose(1,2)  \n",
    "\n",
    "    context_vecs = context_vecs.contiguous().view(batch, num_tokens, self.d_out)\n",
    "    context_vec = self.out_proj(context_vecs)\n",
    "\n",
    "    return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1K75pkGiGc1w"
   },
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 512,\n",
    "    \"emb_dim\": 384,\n",
    "    \"num_heads\": 6,\n",
    "    \"n_layers\": 6,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "TuOLljh1NsHi"
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "  def __init__(self, emb_dim):\n",
    "    super().__init__()\n",
    "    self.eps = 1e-5\n",
    "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "  def forward(self, x):\n",
    "    mean = x.mean(dim=-1, keepdim=True)\n",
    "    var = x.var(dim=-1, keepdim=True, unbiased = False)\n",
    "    norm_x = (x - mean) / torch.sqrt(var + self.eps)       \n",
    "    return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  def forward(self, x):\n",
    "    return 0.5 * x * (1 + torch.tanh(\n",
    "        torch.sqrt(torch.tensor(2.0/torch.pi)) *\n",
    "        (x + 0.044715 * torch.pow(x, 3))\n",
    "    ))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "  def __init__(self, cfg):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "        nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "        GELU(),\n",
    "        nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "n1Zrm_PiNsEa"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self, cfg):\n",
    "    super().__init__()\n",
    "    self.attn = MultiHeadAttention(\n",
    "        d_in = cfg[\"emb_dim\"],\n",
    "        d_out = cfg[\"emb_dim\"],\n",
    "        context_length = cfg[\"context_length\"],\n",
    "        num_heads = cfg[\"num_heads\"],\n",
    "        dropout = cfg[\"drop_rate\"],\n",
    "        qkv_bias = cfg[\"qkv_bias\"]\n",
    "    )\n",
    "    self.ff = FeedForward(cfg)\n",
    "    self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "    self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "    self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "  def forward(self, x):\n",
    "    \n",
    "     short_cut = x\n",
    "     x = self.norm1(x)\n",
    "     x = self.attn(x)\n",
    "     x = self.drop_shortcut(x)\n",
    "     x = x + short_cut  \n",
    "\n",
    "     \n",
    "     short_cut = x  \n",
    "     x = self.norm2(x)\n",
    "     x = self.ff(x)\n",
    "     x = self.drop_shortcut(x)\n",
    "     x = x + short_cut \n",
    "\n",
    "     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "l-gvrPhobHGF"
   },
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "\n",
    "  def __init__(self, cfg):\n",
    "    super().__init__()\n",
    "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "    self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    self.trf_blocks = nn.Sequential(\n",
    "        *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "    )\n",
    "    self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "    self.out_head = nn.Linear(\n",
    "        cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias = False\n",
    "    )\n",
    "\n",
    "  def forward(self, in_idx):\n",
    "    batch_size, seq_len = in_idx.shape\n",
    "    tok_embeds = self.tok_emb(in_idx)\n",
    "    pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device))\n",
    "    x = tok_embeds + pos_embeds\n",
    "    x = self.drop_emb(x)\n",
    "    x = self.trf_blocks(x)\n",
    "    x = self.final_norm(x)\n",
    "    logits = self.out_head(x)\n",
    "\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    \"\"\"Simple text generation function\"\"\"\n",
    "    model.eval()\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "   \n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "       \n",
    "        logits = logits[:, -1, :] \n",
    "        \n",
    "    \n",
    "        probs = torch.softmax(logits, dim=-1) \n",
    "        \n",
    "        \n",
    "        idx_next = torch.argmax(probs, dim=-1, keepdim=True)  \n",
    "        \n",
    "   \n",
    "        idx = torch.cat((idx, idx_next), dim=1)  \n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SNeRw8-VYP1S",
    "outputId": "df94ca88-8146-474d-a712-290b3c7bd856"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) \n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)  \n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "83xezbMfxMqv"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "  def __init__(self, txt, tokenizer, max_length, stride):\n",
    "    self.input_ids = []\n",
    "    self.target_ids = []\n",
    "\n",
    "    token_ids = tokenizer.encode(txt, allowed_special = {\"<|endoftext|>\"})\n",
    "\n",
    "    for i in range(0, len(token_ids)- max_length, stride):\n",
    "      input_chunk = token_ids[i:i + max_length]\n",
    "      target_chunk = token_ids[i+1: i + max_length + 1]\n",
    "\n",
    "      self.input_ids.append(torch.tensor(input_chunk))\n",
    "      self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length = 256,\n",
    "                         stride = 256, shuffle= True, drop_last = True,\n",
    "                         num_workers = 0, pin_memory=None, prefetch_factor=4, persistent_workers=None):\n",
    "\n",
    "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "  dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "\n",
    "  if pin_memory is None:\n",
    "    pin_memory = torch.cuda.is_available()\n",
    "  if persistent_workers is None:\n",
    "    persistent_workers = num_workers > 0\n",
    "\n",
    "  dl_kwargs = {\n",
    "      \"dataset\": dataset,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"shuffle\": shuffle,\n",
    "      \"drop_last\": drop_last,\n",
    "      \"num_workers\": num_workers,\n",
    "      \"pin_memory\": pin_memory,\n",
    "      \"persistent_workers\": persistent_workers,\n",
    "  }\n",
    "  if num_workers and num_workers > 0:\n",
    "      dl_kwargs[\"prefetch_factor\"] = prefetch_factor\n",
    "\n",
    "  dataloader = DataLoader(**dl_kwargs)\n",
    "  return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "pg3lPRPVdZhI"
   },
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "\n",
    "  def __init__(self, cfg):\n",
    "    super().__init__()\n",
    "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "    self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    self.trf_blocks = nn.Sequential(\n",
    "        *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "    )\n",
    "    self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "    self.out_head = nn.Linear(\n",
    "        cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias = False\n",
    "    )\n",
    "\n",
    "  def forward(self, in_idx):\n",
    "    batch_size, seq_len = in_idx.shape\n",
    "    tok_embeds = self.tok_emb(in_idx)\n",
    "    pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device))\n",
    "    x = tok_embeds + pos_embeds\n",
    "    x = self.drop_emb(x)\n",
    "    x = self.trf_blocks(x)\n",
    "    x = self.final_norm(x)\n",
    "    logits = self.out_head(x)\n",
    "\n",
    "    return logits\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NLBlo2zzdZeW"
   },
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "  input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "  logits = model(input_batch)\n",
    "  loss = nn.functional.cross_entropy(logits.flatten(0,1), target_batch.flatten())\n",
    "  return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "  total_loss = 0.\n",
    "  if len(data_loader) == 0:\n",
    "    return float(\"nan\")\n",
    "  elif num_batches is None:\n",
    "    num_batches = len(data_loader)\n",
    "  else:\n",
    "    num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "  for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "    if i < num_batches:\n",
    "      loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "      total_loss += loss.item()\n",
    "    else:\n",
    "      break\n",
    "  return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FNHM9C3edZbg",
    "outputId": "5d1fce40-ec2a-44c6-f3a6-52c073ffb678"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "gO8r6XFxZb1m"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 475,890,377\n",
      "Number of stories: 2,119,719\n",
      "Average tokens per story: 224.5\n"
     ]
    }
   ],
   "source": [
    "train_text_data_str = \" <|endoftext|> \".join(ds['train']['text'])\n",
    "total_tokens = len(tokenizer.encode(train_text_data_str, allowed_special={'<|endoftext|>'}))\n",
    "\n",
    "print(f\"Total tokens: {total_tokens:,}\")\n",
    "print(f\"Number of stories: {len(ds['train']['text']):,}\")\n",
    "print(f\"Average tokens per story: {total_tokens / len(ds['train']['text']):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 4,785,859\n",
      "Number of stories: 21,990\n",
      "Average tokens per story: 217.6\n"
     ]
    }
   ],
   "source": [
    "val_text_data_str = \" <|endoftext|> \".join(ds['validation']['text'])\n",
    "total_tokens = len(tokenizer.encode(val_text_data_str, allowed_special={'<|endoftext|>'}))\n",
    "\n",
    "print(f\"Total tokens: {total_tokens:,}\")\n",
    "print(f\"Number of stories: {len(ds['validation']['text']):,}\")\n",
    "print(f\"Average tokens per story: {total_tokens / len(ds['validation']['text']):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "7d82541b"
   },
   "outputs": [],
   "source": [
    "train_loader_tinystories = create_dataloader_v1(\n",
    "    train_text_data_str,\n",
    "    batch_size=16, \n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=8 \n",
    ")\n",
    "\n",
    "val_loader_tinystories = create_dataloader_v1(\n",
    "    val_text_data_str,\n",
    "    batch_size=16,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=8\n",
    ")\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    train_loader_tinystories.pin_memory = True\n",
    "    val_loader_tinystories.pin_memory = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([16, 512]) torch.Size([16, 512])\n",
      "torch.Size([16, 512]) torch.Size([16, 512])\n",
      "torch.Size([16, 512]) torch.Size([16, 512])\n",
      "torch.Size([16, 512]) torch.Size([16, 512])\n",
      "torch.Size([16, 512]) torch.Size([16, 512])\n",
      "torch.Size([16, 512]) torch.Size([16, 512])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([16, 512]) torch.Size([16, 512])\n",
      "torch.Size([16, 512]) torch.Size([16, 512])\n",
      "torch.Size([16, 512]) torch.Size([16, 512])\n",
      "torch.Size([16, 512]) torch.Size([16, 512])\n",
      "torch.Size([16, 512]) torch.Size([16, 512])\n",
      "torch.Size([16, 512]) torch.Size([16, 512])\n",
      "Number of batches in train_stories: 58092\n",
      "Number of batches in validation_stories: 585\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for i, (x, y) in enumerate(train_loader_tinystories):\n",
    "  print(x.shape, y.shape)\n",
    "  if i == 5:\n",
    "        break\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for i, (x, y) in enumerate(val_loader_tinystories):\n",
    "  print(x.shape, y.shape)\n",
    "  if i == 5:\n",
    "        break\n",
    "\n",
    "print(f\"Number of batches in train_stories: {len(train_loader_tinystories)}\")\n",
    "print(f\"Number of batches in validation_stories: {len(val_loader_tinystories)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer,\n",
    "                       start_epoch=0, global_step=-1,\n",
    "                       train_losses=None, val_losses=None, tokens_seen=0):\n",
    "\n",
    "    import os\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "    if train_losses is None: train_losses = []\n",
    "    if val_losses is None: val_losses = []\n",
    "    if tokens_seen is None: tokens_seen = 0\n",
    "\n",
    "  \n",
    "    global scheduler\n",
    "\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    amp_dtype = torch.bfloat16 if use_cuda and torch.cuda.is_bf16_supported() else torch.float16\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(amp_dtype == torch.float16))\n",
    "\n",
    "    \n",
    "    try:\n",
    "        model = torch.compile(model)  \n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            input_batch, target_batch = input_batch.to(device, non_blocking=True), target_batch.to(device, non_blocking=True)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=use_cuda, dtype=amp_dtype):\n",
    "                loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "\n",
    "            if scaler.is_enabled():\n",
    "                scaler.scale(loss).backward()\n",
    "                # Gradient clipping\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "       \n",
    "            if 'scheduler' in globals() and scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if (global_step % eval_freq) == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Epoch: {epoch+1} | Step: {global_step:06d} | Train loss: {train_loss:.3f} | Val_loss: {val_loss:.3f}\")\n",
    "\n",
    "\n",
    "            if global_step % 50000 == 0:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'train_losses': train_losses,\n",
    "                    'val_losses': val_losses,\n",
    "                    'tokens_seen': tokens_seen,\n",
    "                    'global_step': global_step\n",
    "                }, f'checkpoints/ckpt_step_{global_step}.pt')\n",
    "                print(f\"Checkpoint saved at step {global_step}\")\n",
    "\n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'tokens_seen': tokens_seen,\n",
    "            'global_step': global_step\n",
    "        }, f'checkpoints/ckpt_epoch_{epoch}.pt')\n",
    "        print(f\"Checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "    return train_losses, val_losses, tokens_seen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "  model.eval()\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  amp_dtype = torch.bfloat16 if use_cuda and torch.cuda.is_bf16_supported() else torch.float16\n",
    "  with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(enabled=use_cuda, dtype=amp_dtype):\n",
    "      train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "      val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "  \n",
    "  model.train()\n",
    "  return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  \n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fresh training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_479544/1797713014.py:19: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(amp_dtype == torch.float16))\n",
      "/tmp/ipykernel_479544/1797713014.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_cuda, dtype=amp_dtype):\n",
      "/tmp/ipykernel_479544/1304172257.py:6: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_cuda, dtype=amp_dtype):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Step: 000000 | Train loss: 11.011 | Val_loss: 11.011\n",
      "Checkpoint saved at step 0\n",
      "Once upon a time, there was a little girl namedEhount yawnFirst 1924 Troll Labyrinth orbit Worker HOME Laurel sense residence gate spoon reversible atheistsChip Hispan debut bitcoin Diane poorer humouroga overboard competitors immun NTSSHIPCompan Hasan coaches 311onica Brah sensitivity Task oddly resemblingandraimpl Savannah••Damage Nixonoly uncover fingertips Rac\n",
      "Checkpoint saved at epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_479544/1797713014.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_cuda, dtype=amp_dtype):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at step 50000\n",
      "Once upon a time, there was a little girl named Lily. She loved to play with her toys and run around outside. One day, Lily's mommy asked her to help clean up her toys. Lily didn't want to clean up, but she knew she had to listen to her mommy.\n",
      "Checkpoint saved at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_479544/1304172257.py:6: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_cuda, dtype=amp_dtype):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Step: 100000 | Train loss: 1.607 | Val_loss: 1.620\n",
      "Checkpoint saved at step 100000\n",
      "Once upon a time, there was a little girl named Lily. She loved to play with her toys and her favorite was a big, red ball. One day, Lily's mom asked her to clean up her toys. Lily didn't want to clean up, but she knew she had to listen to her\n",
      "Checkpoint saved at epoch 2\n",
      "Checkpoint saved at step 150000\n",
      "Once upon a time, there was a little girl named Lily. She loved to play outside in the park. One day, she saw a big, scary dog. The dog was barking and running towards her. Lily was scared and ran away.  Later that day, Lily's mom asked her to\n",
      "Checkpoint saved at epoch 3\n",
      "Training completed in 117.97 minutes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\", \"ln.weight\", \"scale\", \"shift\"]\n",
    "param_groups = [\n",
    "    {\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n",
    "    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "]\n",
    "optimizer = torch.optim.AdamW(param_groups, lr=3e-4, betas=(0.9, 0.95))\n",
    "\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def cosine_with_warmup_lambda(current_step, *, warmup_steps, total_steps):\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "  \n",
    "    progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "import math\n",
    "\n",
    "warmup_steps = 2000\n",
    "\n",
    "\n",
    "\n",
    "checkpoint_path = 'checkpoints/ckpt_step_440000.pt'  \n",
    "train_losses, val_losses = [], [] \n",
    "tokens_seen = 0  \n",
    "start_epoch = 0\n",
    "global_step = -1\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    val_losses = checkpoint['val_losses']\n",
    "    tokens_seen = checkpoint['tokens_seen']\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    global_step = checkpoint['global_step']\n",
    "    print(f\"Resumed from checkpoint at epoch {start_epoch}\")\n",
    "else:\n",
    "    print(\"Starting fresh training\")\n",
    "\n",
    "num_epochs = 4\n",
    "\n",
    "\n",
    "estimated_total_steps = max(1, num_epochs * len(train_loader_tinystories))\n",
    "scheduler = LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda=lambda step: cosine_with_warmup_lambda(step, warmup_steps=warmup_steps, total_steps=warmup_steps + estimated_total_steps)\n",
    ")\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader_tinystories, val_loader_tinystories, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=100000, eval_iter=100000,\n",
    "    start_context=\"Once upon a time, there was a little girl named\", tokenizer=tokenizer,\n",
    "    start_epoch=start_epoch, global_step=global_step,\n",
    "    train_losses=train_losses, val_losses=val_losses, tokens_seen=tokens_seen\n",
    ")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1,) and (2,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m     25\u001b[0m epochs_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, num_epochs, \u001b[38;5;28mlen\u001b[39m(train_losses))\n\u001b[0;32m---> 26\u001b[0m \u001b[43mplot_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens_seen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_losses\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 17\u001b[0m, in \u001b[0;36mplot_losses\u001b[0;34m(epochs_seen, tokens_seen, train_losses, val_losses)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Create a second x-axis for tokens seen\u001b[39;00m\n\u001b[1;32m     16\u001b[0m ax2 \u001b[38;5;241m=\u001b[39m ax1\u001b[38;5;241m.\u001b[39mtwiny()  \u001b[38;5;66;03m# Create a second x-axis that shares the same y-axis\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43max2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens_seen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Invisible plot for aligning ticks\u001b[39;00m\n\u001b[1;32m     18\u001b[0m ax2\u001b[38;5;241m.\u001b[39mset_xlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokens seen\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m fig\u001b[38;5;241m.\u001b[39mtight_layout()  \u001b[38;5;66;03m# Adjust layout to make room\u001b[39;00m\n",
      "File \u001b[0;32m~/jupyter-env/lib/python3.10/site-packages/matplotlib/axes/_axes.py:1777\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1536\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1776\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1777\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1779\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/jupyter-env/lib/python3.10/site-packages/matplotlib/axes/_base.py:297\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, return_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 297\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_kwargs\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter-env/lib/python3.10/site-packages/matplotlib/axes/_base.py:494\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    491\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    495\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    498\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1,) and (2,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "   \n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "\n",
    "        if top_k is not None:\n",
    "      \n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "  \n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "       \n",
    "            probs = torch.softmax(logits, dim=-1)  \n",
    "\n",
    "  \n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  \n",
    "\n",
    "       \n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  \n",
    "\n",
    "     \n",
    "        if eos_id is not None and (idx_next == eos_id).any():\n",
    "            break\n",
    "\n",
    "      \n",
    "        idx = torch.cat((idx, idx_next), dim=1)  \n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=200, context_size=context_size, temperature=0.7, top_k=40\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \")) \n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there was a girl named Lucy. She was very curious and loved to explore. One day, she decided to go on an adventure.  She went down a long hill and then she came to a big lake. She wanted to see what was there. She looked around and saw a big tree. It was a very old tree and she wanted to climb it.  So she climbed up the tree. She climbed to the top and looked around. She saw a big lake and she was having so much fun. Then she heard a noise coming from the lake.  She looked around and saw a bird on the ground. It was swimming around!  Lucy started to wonder what the bird was doing. She wanted to help it. She climbed up the tree and carefully climbed down.  The bird was very happy that Lucy was able to help. It made Lucy feel good inside. She said goodbye and flew off into the sky. <|endoftext|> Once upon a time, there was a big lake\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Once upon a time there was a girl named\"\n",
    "\n",
    "generate_and_print_sample(model, tokenizer, device, start_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(549.4410)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity = torch.exp(torch.mean(torch.tensor(train_losses)))\n",
    "perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
